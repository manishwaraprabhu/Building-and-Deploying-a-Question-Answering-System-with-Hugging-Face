# ğŸ§  Building and Deploying a Question Answering System with Hugging Face

## ğŸ“Œ Project Overview  
This project implements an **Extractive Question Answering (QA) System** using Hugging Face Transformers and Streamlit. A pre-trained **DistilBERT** model was fine-tuned on the **SQuAD** dataset to improve accuracy in extracting answers from a given context. The final model was deployed as an interactive **Streamlit** web application for real-time user interaction.

---

## ğŸš€ Project Workflow

### âœ… Step 1: Dataset Preparation
- Loaded and preprocessed the **SQuAD dataset** for fine-tuning.
- Extracted **context, questions, and answers**, ensuring accurate start-end token mappings.
- Converted the data into Hugging Face `Dataset` format for training.
- Saved processed data as `train_data.csv` and `validation_data.csv` for reuse.

---

### âœ… Step 2: Baseline Model Evaluation (Pretrained DistilBERT)
- Loaded `train_data.csv` and `validation_data.csv` for model benchmarking.
- Used **DistilBERT (distilbert-base-cased)** for zero-shot QA evaluation.
- Evaluated performance before fine-tuning to establish baseline metrics.
- Observed limitations in handling domain-specific or complex queries.
- Saved the baseline model as `baseline_model`.

---

### âœ… Step 3: Fine-Tuning the DistilBERT Model
- Loaded the `baseline_model` and fine-tuned it using the Hugging Face **Trainer API**.
- Used **AdamW optimizer** with learning rate scheduling.
- Configured batch size, epochs, and gradient accumulation for optimal performance.
- Trained the model on SQuAD dataset and saved the best version as `best_fine_tuned_model`.

---

### âœ… Step 4: Model Evaluation
- Compared **baseline vs fine-tuned** model performance.
- Evaluated using **Exact Match (EM)** and **F1 Score**.
- Achieved significant improvements post fine-tuning:
  - **EM**: 74.50%
  - **F1**: 83.07%
- Generated `evaluation_report.json` summarizing all key metrics.
- Visualized improvement using bar charts via `matplotlib`.
- Saved incorrect predictions for further error analysis.

---

### âœ… Step 5: Deployment via Streamlit
- Developed a **Streamlit web app** for real-time QA interaction.
- Integrated `best_fine_tuned_model` to power the backend inference.
- Enabled users to input both **context** and **question** for on-the-fly answers.
- Implemented robust error handling for invalid inputs and edge cases.
- Tested thoroughly with real-world examples for stability and usability.

---

## ğŸ“Š Final Results: Performance Comparison

| Model            | Exact Match (EM) | F1 Score  |
|------------------|------------------|-----------|
| Baseline Model   | 71.75%           | 80.65%    |
| Fine-Tuned Model | **74.50%**       | **83.07%**|

âœ… Fine-tuning improved **EM by ~3%** and **F1 Score by ~2.5%**  
âœ… Real-time QA interaction successfully enabled through the deployed UI

---

## âœ… Conclusion  
This project demonstrates the **complete lifecycle of building an NLP solution**â€”from **dataset preparation** and **model fine-tuning** to **deployment and evaluation**. Using Hugging Face Transformers and PyTorch, we improved a DistilBERT modelâ€™s performance on SQuAD and deployed it via **Streamlit**, making it accessible for real-time use cases such as knowledge assistants, intelligent search engines, and chatbot integrations.

---

## ğŸ› ï¸ Technologies & Tools
- Hugging Face Transformers
- PyTorch
- Hugging Face Datasets
- Streamlit
- SQuAD Dataset
- Matplotlib, Pandas
- JSON, CSV, Tokenizers

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_data.csv
â”‚   â””â”€â”€ validation_data.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ baseline_model/
â”‚   â””â”€â”€ best_fine_tuned_model/
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluation_report.json
â”‚   â””â”€â”€ performance_plots.png
â”œâ”€â”€ app.py  # Streamlit Application
â”œâ”€â”€ train.py  # Fine-tuning script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ“– Sample Dataset Entry
```json
{
  "context": "The Amazon rainforest is one of the world's most biodiverse habitats. It plays a critical role in regulating the global climate.",
  "question": "What role does the Amazon rainforest play in the climate?",
  "answer": "regulating the global climate"
}

---

## ğŸ“š Learning Outcomes

- Fine-tuning pre-trained transformer models for extractive QA  
- Understanding tokenization and label alignment for QA  
- Evaluating NLP models using EM and F1 metrics  
- Building and deploying interactive ML apps using Streamlit  
- Exposure to Hugging Face's ecosystem and Trainer API
